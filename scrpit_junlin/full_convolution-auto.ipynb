{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d72e6b-cde1-4088-bed5-f2322065860c",
   "metadata": {},
   "source": [
    "## Entrainement de plusieurs models et plusieurs dataset au choix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34e9d11c-2497-4f1c-8329-94443464fb67",
   "metadata": {},
   "source": [
    "il renvoie un rapport en json\n",
    "le datagen reste toujours \"non\" avant que je fais un fonction pour que le data d'image est en 4 dimension pour adapter le fonction de dataugmentation de keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a801d-6296-4268-9f81-edb9e2313752",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Python Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad95307a-c46b-4e2d-a129-665b604aad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gestion fichier modeles pythons\n",
    "import os , sys            \n",
    "workpath = sys.path[0]\n",
    "\n",
    "sys.path.append(f'{workpath}\\fidle-env\\lib\\site-packages')\n",
    "\n",
    "#modules utilitaires\n",
    "import random as r\n",
    "import numpy as np\n",
    "import time \n",
    "import json\n",
    "import fidle\n",
    "import fidle.pwk as pwk   \n",
    "#traitement image\n",
    "import matplotlib.pyplot as plt   \n",
    "from skimage import io \n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "#module IA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250092b-f23a-405f-9725-444508fcb644",
   "metadata": {},
   "source": [
    "## les diffs choix de dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b002847d-c87a-4e78-b3b4-3fc980feeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = f'{workpath}\\datasheet\\mmClasses-DataBase-IMG\\mmClasses-DataBase-IMG\\data_64_equilibre'\n",
    "descri_datapath = [datapath, 'img_64']\n",
    "run_dir=\"E:\\INSA-cour\\A4A\\Projet_multi\\work-dictory\\\\trained_model\\model_64_equilibre\"\n",
    "fidle.utils.mkdir(run_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead109b-f744-4b83-864a-9d29e8a339ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters for CNN and DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c95a8e-3516-4127-b112-91ba1310b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pourcentage du dataset à load\n",
    "scale = 0.5  \n",
    "\n",
    "batch_size    = 64\n",
    "epochs        = 16\n",
    "fit_verbosity = 1\n",
    "\n",
    "index_to_class = [\"CUBO\",\"DEC\",\"FCC\",\"FCC-sphere\",\"HCP-sphere\",\"ICO\",\"MnBeta_sphere\",\"OH\",\"RTD\",\"BCC\",\"DODECA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118a8ed-37b1-45e8-82ee-fb8f5e552158",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f18a92-c53e-43a1-b62e-4188e1ad0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset (filepath : str):\n",
    "    \"\"\"entrée : chemin vers le dossier contenant la BDD\n",
    "    sortie : listes des images et leur classes respectives, mis à l'échelle (scale) et mélangées\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    au lieu d'avoir un set de training 80%, un set de test 20%, j'ai appliqué ici un strategie conseillé si on a beacoup de donnee, \n",
    "    un training set 70%, un test set pour validation 15%, et un test final set 15% pour faire l'evaluation. Vu que le test set a une effet parasit sur le training set\n",
    "    \"\"\"\n",
    "    \n",
    "    L = os.listdir(filepath)   #liste contenenant le nom (en .jpg) de toutes les images\n",
    "    nb_files = len(L)\n",
    "    nb_files2load = round(nb_files*scale)\n",
    "    size = np.shape(io.imread(f'{filepath}/{L[0]}' , as_gray = True))\n",
    "    \n",
    "    #initialisation listes sortie\n",
    "    img = np.zeros ( (nb_files2load , size[0] , size[1]) , dtype = np.float16 )\n",
    "    ID  = np.zeros ( nb_files2load , dtype = np.int8)\n",
    "   \n",
    "    for i in range (nb_files2load):\n",
    "        \n",
    "        random_index = r.randint(0 , nb_files - i - 1)\n",
    "        img[i] = io.imread (f'{filepath}/{L[random_index]}',as_gray = True)\n",
    "        ID [i] = int (L[random_index][:3]) - 1\n",
    "        L.pop(random_index)\n",
    "        \n",
    "    N = len(img)\n",
    "    nb_img = N\n",
    "    a = N*0.7\n",
    "    b = N*0.85\n",
    "    img_train = img[0:round(a)]\n",
    "    img_test  = img[round(a):round(b)]\n",
    "    img_test_final = img[round(b):]\n",
    "    ID_train = ID[0:round(a)]\n",
    "    ID_test  = ID[round(a):round(b)]  \n",
    "    ID_test_final = ID[round(b):]\n",
    "    \n",
    "        \n",
    "    return img_train,img_test,ID_train,ID_test,nb_img, img_test_final, ID_test_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21a969-2e4b-43d1-b81a-e7dbccd98fe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models creations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371cfdd-3c96-4d0c-84f1-59812bdde01e",
   "metadata": {},
   "source": [
    "## comparaison des functions d'activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859844b0-8792-4819-b52b-3e30776ca463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model1(lx,ly): \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( keras.layers.Dense(1000, activation='relu'))\n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    \n",
    "    code_model='1'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "\n",
    "def create_model1_DNNsigmod(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.sigmoid))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "              \n",
    "    code_model='1_DNNsigmod'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "              \n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "def create_model1_DNNtanh(lx,ly): \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.tanh))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNtanh'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model1_DNNsoftsign(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "              \n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.softsign))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNsoftsign'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "def create_model1_DNNelu(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.elu))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNelu'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model              \n",
    "\n",
    "\n",
    "              \n",
    "def create_model1_DNNselu(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.selu))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNselu'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model                \n",
    "      \n",
    "def create_model1_DNNsoftplus(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.softplus))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNsoftplus'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8355078-310d-44b0-8702-20afa442f1a1",
   "metadata": {},
   "source": [
    "## nombre de couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab24b3-0fe4-438a-9ccb-ba16c6aa2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1_100(lx,ly): # 1 couche de dense de 100 neurons\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( keras.layers.Dense(100, activation='relu'))\n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_100'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model2_100(lx,ly): #3_0 2 couche de dense de 100 neurons\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( keras.layers.Dense(100, activation='relu'))\n",
    "    model.add( keras.layers.Dense(100, activation='relu'))\n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "  \n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='2_100'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model3_100(lx,ly): # 3 couche de dense, 100 neurons chacun\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( keras.layers.Dense(100, activation='relu'))\n",
    "    model.add( keras.layers.Dense(100, activation='relu'))\n",
    "    model.add( keras.layers.Dense(100, activation='relu'))\n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='3_100'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model , model_name, code_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69e54e-a901-4fe0-b7a2-ae86676b88f9",
   "metadata": {},
   "source": [
    "## diffs optimiseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59aedac1-04ff-4eb8-aed0-a512de9515f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model1_DNNtanh_adam(lx,ly): \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.tanh))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='1_DNNtanh_adam'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model1_DNNtanh_Adamax(lx,ly): \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.tanh))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adamax', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='1_DNNtanh_Adamax'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "def create_model1_DNNtanh_Nadam(lx,ly): \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.tanh))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='1_DNNtanh_Nadam'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model1_DNNtanh_RMSprop(lx,ly): \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.tanh))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='1_DNNtanh_RMSprop'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d7720-ac76-4f80-b03f-a1b9e840efa5",
   "metadata": {},
   "source": [
    "## nombre de neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132e6ba0-274c-420f-b2f0-410aac182d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2_100(lx,ly): # 2 couche de dense de 100 neurons\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(100))\n",
    "    model.add( layers.Dense(100))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_100'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model2_300(lx,ly): # 2 couche de dense de 300 neurons\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(300))\n",
    "    model.add( layers.Dense(300))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_300'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model2_500(lx,ly): # 2 couche de dense de 500 neurons\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(500))\n",
    "    model.add( layers.Dense(500))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_500'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model2_700(lx,ly): # 2 couche de dense de 700 neurons\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(700))\n",
    "    model.add( layers.Dense(700))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_700'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model2_900(lx,ly): # 2 couche de dense de 900 neurons\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(900))\n",
    "    model.add( layers.Dense(900))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_900'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "def create_model2_1000(lx,ly): # 2 couche de dense de 700 neurons\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_1000'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06856142-3d21-47a8-b713-5b8a5040db30",
   "metadata": {},
   "source": [
    "## qq variable a entrer dans les foncitons suivants (n'oublier pas de changer tag_id si on change de modele, dataset, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee00af3-bac1-4ad3-a331-cd35678497c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# les entree pour le dataset 64\n",
    "datasets = [descri_datapath]\n",
    "with_datagen  = False\n",
    "t = pwk.tag_now()\n",
    "tag_id = f'nombre_neurons_{str(t)}' #si on faire un autre execution de script, n'oublie pas de changer le tag_id comme \"fonc_acti_<un date>\" ou \"nombre_de_couche\", etc.\n",
    "#tag_id = 'optimizer_08_03_v2'\n",
    "#en fait, on peut lance plusieurs fois les meme entrainements, pour surperposer les courbes sur le meme schema dans tensorboard.\n",
    "#models = ['create_model1','create_model1_DNNsigmod','create_model1_DNNtanh','create_model1_DNNsoftsign','create_model1_DNNelu','create_model1_DNNselu','create_model1_DNNsoftplus']\n",
    "#models = ['create_model1_100','create_model2_100','create_model3_100']\n",
    "#models = ['create_model1_DNNtanh_adam','create_model1_DNNtanh_Adamax','create_model1_DNNtanh_Nadam','create_model1_DNNtanh_RMSprop']\n",
    "models = ['create_model2_100','create_model2_300','create_model2_500','create_model2_700','create_model2_900','create_model2_1000']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762c38b-b783-41bf-b6e8-45bba5c0ada4",
   "metadata": {},
   "source": [
    "## Multiple datasets, multiple models ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed889bec-a219-4311-8094-f62ca32b9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_run(datasets, models, datagen=None,\n",
    "              scale=1, batch_size=64, epochs=16, \n",
    "              fit_verbosity=0, tag_id='last'):\n",
    "    \"\"\"\n",
    "    Launches a dataset-model combination\n",
    "    args:\n",
    "        enhanced_dir   : Directory of the enhanced datasets\n",
    "        datasets       : List of dataset (whitout .h5)\n",
    "        models         : List of model like { \"model name\":get_model(), ...}\n",
    "        datagen        : Data generator or None (None)\n",
    "        scale          : % of dataset to use.  1 mean all. (1)\n",
    "        batch_size     : Batch size (64)\n",
    "        epochs         : Number of epochs (16)\n",
    "        fit_verbosity  : Verbose level (0)\n",
    "        tag_id         : postfix for report, logs and models dir (_last)\n",
    "    return:\n",
    "        report        : Report as a dict for Pandas.\n",
    "    \"\"\"  \n",
    "    # ---- Logs and models dir\n",
    "    #\n",
    "    os.makedirs(f'{run_dir}/logs_{tag_id}',   mode=0o750, exist_ok=True)\n",
    "    os.makedirs(f'{run_dir}/models_{tag_id}', mode=0o750, exist_ok=True)\n",
    "    \n",
    "    # ---- Columns of output\n",
    "    #\n",
    "    output={}\n",
    "    output['Dataset'] = []\n",
    "    output['Size']    = []\n",
    "    \n",
    "    for m in models:\n",
    "        output[m+'_Accuracy'] = []\n",
    "        output[m+'_Duration'] = []\n",
    "        output[m+'_loss'] = []\n",
    "    # ---- Let's go\n",
    "    #\n",
    "    for d_name in datasets:\n",
    "        print(\"\\nDataset : \",d_name[0])\n",
    "        \n",
    "        # ---- Read dataset\n",
    "        img_train,img_test,ID_train,ID_test, d_size, img_test_final, ID_test_final = read_dataset(d_name[0])\n",
    "        d_name = d_name[1]\n",
    "        output['Dataset'].append(d_name)\n",
    "        output['Size'].append(d_size)\n",
    "        print(len(img_train))\n",
    "        print(len(ID_train))\n",
    "        # ---- Rescale c'est pas vraiment necessaire\n",
    "        ##img_train,ID_train,img_test,ID_test = pwk.rescale_dataset(img_train,ID_train,img_test,ID_test, scale=scale)\n",
    "        \n",
    "        # ---- Get the shape\n",
    "        (n,lx,ly) = img_train.shape\n",
    "\n",
    "        # ---- For each model\n",
    "        for m_function in models:\n",
    "            \n",
    "            # ---- get model\n",
    "            try:\n",
    "                # ---- get function by name\n",
    "                m = m_function\n",
    "                m_function=globals()[m_function]\n",
    "                model, m_name, code_model=m_function(lx,ly)\n",
    "                \n",
    "                print(\"    Run model {}  : \".format(code_model), end='')\n",
    "                # ---- Compile it # si on ne compare pas de diffs optimizer on ajoute ce ligne.\n",
    "                #model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                # ---- Callbacks tensorboard\n",
    "                log_dir = f'{run_dir}/logs_{tag_id}/tb_{d_name}_{m_name}'\n",
    "                tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                # ---- Callbacks bestmodel\n",
    "                save_dir = f'{run_dir}/models_{tag_id}/model_{d_name}_{m_name}.h5'\n",
    "                bestmodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, monitor='accuracy', save_best_only=True)\n",
    "                # ---- Train\n",
    "                start_time = time.time()\n",
    "                if datagen==None:\n",
    "                    # ---- No data augmentation (datagen=None) --------------------------------------\n",
    "                    history = model.fit(img_train, ID_train,\n",
    "                                        batch_size      = batch_size,\n",
    "                                        epochs          = epochs,\n",
    "                                        verbose         = fit_verbosity,\n",
    "                                        validation_data = (img_test, ID_test),\n",
    "                                        callbacks       = [tensorboard_callback, bestmodel_callback])\n",
    "                else:\n",
    "                    # ---- Data augmentation (datagen given) ----------------------------------------\n",
    "                    datagen.fit(x_train)\n",
    "                    history = model.fit(datagen.flow(img_train, ID_train, batch_size=batch_size),\n",
    "                                        steps_per_epoch = int(len(img_train)/batch_size),\n",
    "                                        epochs          = epochs,\n",
    "                                        verbose         = fit_verbosity,\n",
    "                                        validation_data = (img_test, ID_test),\n",
    "                                        callbacks       = [tensorboard_callback, bestmodel_callback])\n",
    "                \n",
    "                \n",
    "                \n",
    "                # ---- Result\n",
    "                end_time = time.time()\n",
    "                duration = end_time-start_time\n",
    "                # ---- Evaluation final\n",
    "                loss, accuracy = model.evaluate(img_test_final, ID_test_final, batch_size=batch_size)\n",
    "                accuracy = accuracy * 100\n",
    "                #accuracy = max(history.history[\"val_accuracy\"])*100\n",
    "                #\n",
    "                output[m+'_Accuracy'].append(accuracy)\n",
    "                output[m+'_Duration'].append(duration)\n",
    "                output[m+'_loss'].append(loss)\n",
    "                print(f\"Accuracy={accuracy: 7.2f}    Duration={duration: 7.2f}\")\n",
    "            except:\n",
    "                raise\n",
    "                output[m+'_Accuracy'].append('0')\n",
    "                output[m+'_Duration'].append('999')\n",
    "                print('-')\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b497c82-e1b9-49cd-89e8-eac20d57eabb",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c63152-81e5-4563-83ca-956d8b9687b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Run --------------------------------------------------\n",
      "\n",
      "Dataset :  E:\\INSA-cour\\A4A\\Projet_multi\\work-dictory\\datasheet\\mmClasses-DataBase-IMG\\mmClasses-DataBase-IMG\\data_64_equilibre\n",
      "7015\n",
      "7015\n",
      "    Run model 2_100  : Epoch 1/16\n",
      "110/110 [==============================] - 13s 117ms/step - loss: 2.2823 - accuracy: 0.1531 - val_loss: 2.0859 - val_accuracy: 0.2128\n",
      "Epoch 2/16\n",
      "110/110 [==============================] - 12s 110ms/step - loss: 1.8755 - accuracy: 0.2837 - val_loss: 1.5376 - val_accuracy: 0.3916\n",
      "Epoch 3/16\n",
      "110/110 [==============================] - 12s 113ms/step - loss: 1.4655 - accuracy: 0.4151 - val_loss: 1.1616 - val_accuracy: 0.5519\n",
      "Epoch 4/16\n",
      "110/110 [==============================] - 12s 112ms/step - loss: 1.2129 - accuracy: 0.5036 - val_loss: 1.0811 - val_accuracy: 0.5898\n",
      "Epoch 5/16\n",
      "110/110 [==============================] - 12s 112ms/step - loss: 1.0323 - accuracy: 0.5919 - val_loss: 0.8237 - val_accuracy: 0.6662\n",
      "Epoch 6/16\n",
      "110/110 [==============================] - 12s 111ms/step - loss: 0.8915 - accuracy: 0.6489 - val_loss: 0.6933 - val_accuracy: 0.7128\n",
      "Epoch 7/16\n",
      "110/110 [==============================] - 13s 115ms/step - loss: 0.7915 - accuracy: 0.6887 - val_loss: 0.6692 - val_accuracy: 0.7134\n",
      "Epoch 8/16\n",
      "110/110 [==============================] - 13s 115ms/step - loss: 0.7123 - accuracy: 0.7243 - val_loss: 0.5173 - val_accuracy: 0.8025\n",
      "Epoch 9/16\n",
      "110/110 [==============================] - 12s 110ms/step - loss: 0.6546 - accuracy: 0.7521 - val_loss: 0.5134 - val_accuracy: 0.7972\n",
      "Epoch 10/16\n",
      "110/110 [==============================] - 13s 123ms/step - loss: 0.5665 - accuracy: 0.7837 - val_loss: 0.4307 - val_accuracy: 0.8344\n",
      "Epoch 11/16\n",
      "110/110 [==============================] - 12s 108ms/step - loss: 0.5237 - accuracy: 0.8063 - val_loss: 0.3494 - val_accuracy: 0.8717\n",
      "Epoch 12/16\n",
      "110/110 [==============================] - 12s 107ms/step - loss: 0.4947 - accuracy: 0.8121 - val_loss: 0.4187 - val_accuracy: 0.8398\n",
      "Epoch 13/16\n",
      "110/110 [==============================] - 12s 108ms/step - loss: 0.4456 - accuracy: 0.8362 - val_loss: 0.3364 - val_accuracy: 0.8750\n",
      "Epoch 14/16\n",
      "110/110 [==============================] - 12s 107ms/step - loss: 0.4032 - accuracy: 0.8462 - val_loss: 0.2628 - val_accuracy: 0.9029\n",
      "Epoch 15/16\n",
      "110/110 [==============================] - 12s 112ms/step - loss: 0.4089 - accuracy: 0.8468 - val_loss: 0.2839 - val_accuracy: 0.8883\n",
      "Epoch 16/16\n",
      "110/110 [==============================] - 13s 116ms/step - loss: 0.3642 - accuracy: 0.8644 - val_loss: 0.2739 - val_accuracy: 0.8870\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 0.2914 - accuracy: 0.8922\n",
      "Accuracy=  89.22    Duration= 198.47\n",
      "    Run model 2_300  : Epoch 1/16\n",
      "110/110 [==============================] - 13s 111ms/step - loss: 2.2231 - accuracy: 0.1725 - val_loss: 1.9602 - val_accuracy: 0.2786\n",
      "Epoch 2/16\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 1.6687 - accuracy: 0.3491 - val_loss: 1.2855 - val_accuracy: 0.4614\n",
      "Epoch 3/16\n",
      "110/110 [==============================] - 13s 114ms/step - loss: 1.2560 - accuracy: 0.4855 - val_loss: 0.9603 - val_accuracy: 0.6303\n",
      "Epoch 4/16\n",
      "110/110 [==============================] - 12s 112ms/step - loss: 1.0493 - accuracy: 0.5716 - val_loss: 0.8386 - val_accuracy: 0.6656\n",
      "Epoch 5/16\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 0.9244 - accuracy: 0.6304 - val_loss: 0.8487 - val_accuracy: 0.6383\n",
      "Epoch 6/16\n",
      "110/110 [==============================] - 13s 114ms/step - loss: 0.7689 - accuracy: 0.6912 - val_loss: 0.6556 - val_accuracy: 0.7281\n",
      "Epoch 7/16\n",
      "110/110 [==============================] - 13s 118ms/step - loss: 0.6695 - accuracy: 0.7323 - val_loss: 0.5189 - val_accuracy: 0.8085\n",
      "Epoch 8/16\n",
      "110/110 [==============================] - 13s 119ms/step - loss: 0.6107 - accuracy: 0.7604 - val_loss: 0.5007 - val_accuracy: 0.8012\n",
      "Epoch 9/16\n",
      "110/110 [==============================] - 14s 123ms/step - loss: 0.5452 - accuracy: 0.7903 - val_loss: 0.3837 - val_accuracy: 0.8584\n",
      "Epoch 10/16\n",
      "110/110 [==============================] - 14s 125ms/step - loss: 0.5176 - accuracy: 0.7980 - val_loss: 0.4575 - val_accuracy: 0.8178\n",
      "Epoch 11/16\n",
      "110/110 [==============================] - 13s 118ms/step - loss: 0.4340 - accuracy: 0.8351 - val_loss: 0.3808 - val_accuracy: 0.8570\n",
      "Epoch 12/16\n",
      "110/110 [==============================] - 14s 123ms/step - loss: 0.4186 - accuracy: 0.8395 - val_loss: 0.3203 - val_accuracy: 0.8790\n",
      "Epoch 13/16\n",
      "110/110 [==============================] - 13s 118ms/step - loss: 0.3825 - accuracy: 0.8488 - val_loss: 0.3053 - val_accuracy: 0.8843\n",
      "Epoch 14/16\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 0.3530 - accuracy: 0.8646 - val_loss: 0.3469 - val_accuracy: 0.8637\n",
      "Epoch 15/16\n",
      "110/110 [==============================] - 13s 115ms/step - loss: 0.3191 - accuracy: 0.8774 - val_loss: 0.2689 - val_accuracy: 0.8930\n",
      "Epoch 16/16\n",
      "110/110 [==============================] - 13s 116ms/step - loss: 0.3215 - accuracy: 0.8793 - val_loss: 0.2535 - val_accuracy: 0.9003\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 0.2896 - accuracy: 0.8856\n",
      "Accuracy=  88.56    Duration= 208.25\n",
      "    Run model 2_500  : Epoch 1/16\n",
      "110/110 [==============================] - 14s 119ms/step - loss: 2.2008 - accuracy: 0.1793 - val_loss: 1.8539 - val_accuracy: 0.2852\n",
      "Epoch 2/16\n",
      "110/110 [==============================] - 15s 139ms/step - loss: 1.6148 - accuracy: 0.3682 - val_loss: 1.2836 - val_accuracy: 0.4621\n",
      "Epoch 3/16\n",
      "110/110 [==============================] - 15s 134ms/step - loss: 1.2198 - accuracy: 0.5129 - val_loss: 1.0202 - val_accuracy: 0.5824\n",
      "Epoch 4/16\n",
      "110/110 [==============================] - 13s 118ms/step - loss: 1.0304 - accuracy: 0.5879 - val_loss: 0.8386 - val_accuracy: 0.6815\n",
      "Epoch 5/16\n",
      "110/110 [==============================] - 14s 126ms/step - loss: 0.8944 - accuracy: 0.6405 - val_loss: 0.7123 - val_accuracy: 0.7194\n",
      "Epoch 6/16\n",
      "110/110 [==============================] - 14s 123ms/step - loss: 0.7386 - accuracy: 0.7014 - val_loss: 0.5165 - val_accuracy: 0.8271\n",
      "Epoch 7/16\n",
      "110/110 [==============================] - 14s 124ms/step - loss: 0.6714 - accuracy: 0.7387 - val_loss: 0.4933 - val_accuracy: 0.8138\n",
      "Epoch 8/16\n",
      "110/110 [==============================] - 13s 122ms/step - loss: 0.6090 - accuracy: 0.7703 - val_loss: 0.5198 - val_accuracy: 0.8078\n",
      "Epoch 9/16\n",
      "110/110 [==============================] - 14s 126ms/step - loss: 0.5130 - accuracy: 0.8016 - val_loss: 0.4067 - val_accuracy: 0.8391\n",
      "Epoch 10/16\n",
      "110/110 [==============================] - 13s 115ms/step - loss: 0.4967 - accuracy: 0.8096 - val_loss: 0.3808 - val_accuracy: 0.8551\n",
      "Epoch 11/16\n",
      "110/110 [==============================] - 14s 126ms/step - loss: 0.4415 - accuracy: 0.8331 - val_loss: 0.3365 - val_accuracy: 0.8684\n",
      "Epoch 12/16\n",
      "110/110 [==============================] - 15s 138ms/step - loss: 0.4073 - accuracy: 0.8500 - val_loss: 0.3327 - val_accuracy: 0.8590\n",
      "Epoch 13/16\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 0.3767 - accuracy: 0.8573 - val_loss: 0.3192 - val_accuracy: 0.8783\n",
      "Epoch 14/16\n",
      "110/110 [==============================] - 13s 118ms/step - loss: 0.3553 - accuracy: 0.8661 - val_loss: 0.2683 - val_accuracy: 0.8923\n",
      "Epoch 15/16\n",
      "110/110 [==============================] - 14s 129ms/step - loss: 0.3403 - accuracy: 0.8693 - val_loss: 0.2675 - val_accuracy: 0.9003\n",
      "Epoch 16/16\n",
      "110/110 [==============================] - 14s 129ms/step - loss: 0.3220 - accuracy: 0.8781 - val_loss: 0.2520 - val_accuracy: 0.9062\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.2753 - accuracy: 0.8942\n",
      "Accuracy=  89.42    Duration= 221.51\n",
      "    Run model 2_700  : Epoch 1/16\n",
      "110/110 [==============================] - 22s 192ms/step - loss: 2.1757 - accuracy: 0.1828 - val_loss: 1.9364 - val_accuracy: 0.2892\n",
      "Epoch 2/16\n",
      "110/110 [==============================] - 19s 170ms/step - loss: 1.5392 - accuracy: 0.3802 - val_loss: 1.1114 - val_accuracy: 0.5419\n",
      "Epoch 3/16\n",
      "110/110 [==============================] - 19s 171ms/step - loss: 1.1634 - accuracy: 0.5307 - val_loss: 0.8405 - val_accuracy: 0.6789\n",
      "Epoch 4/16\n",
      "110/110 [==============================] - 20s 185ms/step - loss: 0.9883 - accuracy: 0.6040 - val_loss: 0.7767 - val_accuracy: 0.7021\n",
      "Epoch 5/16\n",
      "110/110 [==============================] - 17s 158ms/step - loss: 0.8454 - accuracy: 0.6650 - val_loss: 0.6067 - val_accuracy: 0.7586\n",
      "Epoch 6/16\n",
      "110/110 [==============================] - 15s 134ms/step - loss: 0.7191 - accuracy: 0.7118 - val_loss: 0.5447 - val_accuracy: 0.7899\n",
      "Epoch 7/16\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 0.6643 - accuracy: 0.7397 - val_loss: 0.5134 - val_accuracy: 0.8145\n",
      "Epoch 8/16\n",
      "110/110 [==============================] - 16s 148ms/step - loss: 0.5761 - accuracy: 0.7745 - val_loss: 0.4486 - val_accuracy: 0.8225\n",
      "Epoch 9/16\n",
      "110/110 [==============================] - 15s 139ms/step - loss: 0.5456 - accuracy: 0.7923 - val_loss: 0.4426 - val_accuracy: 0.8265\n",
      "Epoch 10/16\n",
      "110/110 [==============================] - 16s 150ms/step - loss: 0.4858 - accuracy: 0.8148 - val_loss: 0.3406 - val_accuracy: 0.8703\n",
      "Epoch 11/16\n",
      "110/110 [==============================] - 13s 115ms/step - loss: 0.4407 - accuracy: 0.8336 - val_loss: 0.3995 - val_accuracy: 0.8504\n",
      "Epoch 12/16\n",
      "110/110 [==============================] - 13s 114ms/step - loss: 0.4100 - accuracy: 0.8476 - val_loss: 0.4423 - val_accuracy: 0.8351\n",
      "Epoch 13/16\n",
      "110/110 [==============================] - 17s 159ms/step - loss: 0.4037 - accuracy: 0.8473 - val_loss: 0.3353 - val_accuracy: 0.8690\n",
      "Epoch 14/16\n",
      "110/110 [==============================] - 18s 166ms/step - loss: 0.4026 - accuracy: 0.8490 - val_loss: 0.3045 - val_accuracy: 0.8863\n",
      "Epoch 15/16\n",
      "110/110 [==============================] - 16s 149ms/step - loss: 0.3568 - accuracy: 0.8602 - val_loss: 0.3460 - val_accuracy: 0.8644\n",
      "Epoch 16/16\n",
      "110/110 [==============================] - 13s 121ms/step - loss: 0.3244 - accuracy: 0.8805 - val_loss: 0.2479 - val_accuracy: 0.9076\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 0.3074 - accuracy: 0.8876\n",
      "Accuracy=  88.76    Duration= 268.09\n",
      "    Run model 2_900  : Epoch 1/16\n",
      "110/110 [==============================] - 15s 132ms/step - loss: 2.2275 - accuracy: 0.1668 - val_loss: 1.8241 - val_accuracy: 0.2733\n",
      "Epoch 2/16\n",
      "110/110 [==============================] - 13s 117ms/step - loss: 1.5925 - accuracy: 0.3671 - val_loss: 1.2729 - val_accuracy: 0.4934\n",
      "Epoch 3/16\n",
      "110/110 [==============================] - 13s 117ms/step - loss: 1.1722 - accuracy: 0.5284 - val_loss: 0.9728 - val_accuracy: 0.5944\n",
      "Epoch 4/16\n",
      "110/110 [==============================] - 13s 122ms/step - loss: 0.9922 - accuracy: 0.6033 - val_loss: 0.7735 - val_accuracy: 0.6642\n",
      "Epoch 5/16\n",
      "110/110 [==============================] - 13s 118ms/step - loss: 0.8727 - accuracy: 0.6492 - val_loss: 0.7065 - val_accuracy: 0.7128\n",
      "Epoch 6/16\n",
      "110/110 [==============================] - 13s 121ms/step - loss: 0.7236 - accuracy: 0.7071 - val_loss: 0.6168 - val_accuracy: 0.7493\n",
      "Epoch 7/16\n",
      "110/110 [==============================] - 13s 117ms/step - loss: 0.6920 - accuracy: 0.7226 - val_loss: 0.4395 - val_accuracy: 0.8398\n",
      "Epoch 8/16\n",
      "110/110 [==============================] - 13s 122ms/step - loss: 0.6133 - accuracy: 0.7692 - val_loss: 0.5181 - val_accuracy: 0.7952\n",
      "Epoch 9/16\n",
      "110/110 [==============================] - 13s 119ms/step - loss: 0.5668 - accuracy: 0.7840 - val_loss: 0.4109 - val_accuracy: 0.8351\n",
      "Epoch 10/16\n",
      "110/110 [==============================] - 13s 117ms/step - loss: 0.5025 - accuracy: 0.8030 - val_loss: 0.4268 - val_accuracy: 0.8344\n",
      "Epoch 11/16\n",
      "110/110 [==============================] - 13s 118ms/step - loss: 0.4463 - accuracy: 0.8295 - val_loss: 0.3998 - val_accuracy: 0.8551\n",
      "Epoch 12/16\n",
      "110/110 [==============================] - 13s 117ms/step - loss: 0.4222 - accuracy: 0.8356 - val_loss: 0.2902 - val_accuracy: 0.8890\n",
      "Epoch 13/16\n",
      "110/110 [==============================] - 14s 124ms/step - loss: 0.3939 - accuracy: 0.8475 - val_loss: 0.3074 - val_accuracy: 0.8843\n",
      "Epoch 14/16\n",
      "110/110 [==============================] - 19s 176ms/step - loss: 0.3753 - accuracy: 0.8542 - val_loss: 0.3251 - val_accuracy: 0.8777\n",
      "Epoch 15/16\n",
      "110/110 [==============================] - 14s 127ms/step - loss: 0.3428 - accuracy: 0.8684 - val_loss: 0.3063 - val_accuracy: 0.8823\n",
      "Epoch 16/16\n",
      "110/110 [==============================] - 13s 116ms/step - loss: 0.3022 - accuracy: 0.8858 - val_loss: 0.2880 - val_accuracy: 0.8956\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.3174 - accuracy: 0.8829\n",
      "Accuracy=  88.29    Duration= 218.27\n",
      "    Run model 2_1000  : Epoch 1/16\n",
      "110/110 [==============================] - 16s 137ms/step - loss: 2.2041 - accuracy: 0.1725 - val_loss: 1.8041 - val_accuracy: 0.2872\n",
      "Epoch 2/16\n",
      "110/110 [==============================] - 21s 187ms/step - loss: 1.5517 - accuracy: 0.3819 - val_loss: 1.1288 - val_accuracy: 0.5445\n",
      "Epoch 3/16\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 1.2095 - accuracy: 0.5079 - val_loss: 1.0132 - val_accuracy: 0.6031\n",
      "Epoch 4/16\n",
      "110/110 [==============================] - 15s 133ms/step - loss: 0.9852 - accuracy: 0.6071 - val_loss: 0.7538 - val_accuracy: 0.6948\n",
      "Epoch 5/16\n",
      "110/110 [==============================] - 14s 124ms/step - loss: 0.8743 - accuracy: 0.6627 - val_loss: 0.6778 - val_accuracy: 0.7507\n",
      "Epoch 6/16\n",
      "110/110 [==============================] - 14s 126ms/step - loss: 0.7336 - accuracy: 0.7088 - val_loss: 0.6150 - val_accuracy: 0.7706\n",
      "Epoch 7/16\n",
      "110/110 [==============================] - 13s 119ms/step - loss: 0.6992 - accuracy: 0.7259 - val_loss: 0.5626 - val_accuracy: 0.7786\n",
      "Epoch 8/16\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 0.6164 - accuracy: 0.7591 - val_loss: 0.4404 - val_accuracy: 0.8471\n",
      "Epoch 9/16\n",
      "110/110 [==============================] - 13s 119ms/step - loss: 0.5477 - accuracy: 0.7944 - val_loss: 0.4414 - val_accuracy: 0.8271\n",
      "Epoch 10/16\n",
      "110/110 [==============================] - 13s 121ms/step - loss: 0.5092 - accuracy: 0.8024 - val_loss: 0.3834 - val_accuracy: 0.8597\n",
      "Epoch 11/16\n",
      "110/110 [==============================] - 14s 131ms/step - loss: 0.4687 - accuracy: 0.8231 - val_loss: 0.3574 - val_accuracy: 0.8570\n",
      "Epoch 12/16\n",
      "110/110 [==============================] - 13s 121ms/step - loss: 0.4417 - accuracy: 0.8318 - val_loss: 0.3455 - val_accuracy: 0.8757\n",
      "Epoch 13/16\n",
      "110/110 [==============================] - 13s 120ms/step - loss: 0.3968 - accuracy: 0.8506 - val_loss: 0.3380 - val_accuracy: 0.8684\n",
      "Epoch 14/16\n",
      "110/110 [==============================] - 14s 123ms/step - loss: 0.3840 - accuracy: 0.8539 - val_loss: 0.2658 - val_accuracy: 0.8923\n",
      "Epoch 15/16\n",
      "110/110 [==============================] - 13s 121ms/step - loss: 0.3507 - accuracy: 0.8686 - val_loss: 0.2830 - val_accuracy: 0.8923\n",
      "Epoch 16/16\n",
      "110/110 [==============================] - 14s 124ms/step - loss: 0.3573 - accuracy: 0.8669 - val_loss: 0.2540 - val_accuracy: 0.9009\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 0.2729 - accuracy: 0.9088\n",
      "Accuracy=  90.88    Duration= 233.26\n",
      "\n",
      "Report saved as  E:\\INSA-cour\\A4A\\Projet_multi\\work-dictory\\trained_model\\model_64_equilibre/report_nombre_neurons_2023-03-12_09h20m20s.json\n",
      "\n",
      "Duration :  00:22:41 776ms\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pwk.chrono_start()\n",
    "\n",
    "print('\\n---- Run','-'*50)\n",
    "\n",
    "\n",
    "# ---- Data augmentation or not\n",
    "#\n",
    "if with_datagen :\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n",
    "                                                           featurewise_std_normalization=False,\n",
    "                                                           width_shift_range=0.1,\n",
    "                                                           height_shift_range=0.1,\n",
    "                                                           zoom_range=0.2,\n",
    "                                                           shear_range=0.1,\n",
    "                                                           rotation_range=10.)\n",
    "else:\n",
    "    datagen=None\n",
    "    \n",
    "# ---- Run\n",
    "#\n",
    "output = multi_run(datasets, \n",
    "                   models,\n",
    "                   datagen       = datagen,\n",
    "                   scale         = scale,\n",
    "                   batch_size    = batch_size,\n",
    "                   epochs        = epochs,\n",
    "                   fit_verbosity = fit_verbosity,\n",
    "                   tag_id        = tag_id)\n",
    "\n",
    "# ---- Save report\n",
    "#\n",
    "report={}\n",
    "report['output']=output\n",
    "report['description'] = f'scale={scale} batch_size={batch_size} epochs={epochs} data_aug={with_datagen}'\n",
    "\n",
    "report_name=f'{run_dir}/report_{tag_id}.json'\n",
    "\n",
    "with open(report_name, 'w') as file:\n",
    "    json.dump(report, file, indent=4)\n",
    "\n",
    "print('\\nReport saved as ',report_name)\n",
    "\n",
    "pwk.chrono_show()\n",
    "print('-'*59)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582c42f-798c-4ee4-b422-e4202e28b268",
   "metadata": {},
   "source": [
    "## show report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5807c0-9280-45e6-98b9-c83193db1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dir = run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097f6b7f-873e-4844-82e9-790cd6a3c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = (s == s.max())\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "def show_report(file):\n",
    "    # ---- Read json file\n",
    "    with open(file) as infile:\n",
    "        dict_report = json.load( infile )\n",
    "    output      = dict_report['output']\n",
    "    description = dict_report['description']\n",
    "    # ---- about\n",
    "    pwk.subtitle(f'Report : {Path(file).stem}')\n",
    "    print(    \"Desc.  : \",description,'\\n')\n",
    "    # ---- Create a pandas\n",
    "    report       = pd.DataFrame (output)\n",
    "    col_accuracy = [ c for c in output.keys() if c.endswith('Accuracy')]\n",
    "    col_duration = [ c for c in output.keys() if c.endswith('Duration')]\n",
    "    # ---- Build formats\n",
    "    lambda_acc = lambda x : '{:.2f} %'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n",
    "    lambda_dur = lambda x : '{:.1f} s'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n",
    "    formats = {'Size':'{:.2f} Mo'}\n",
    "    for c in col_accuracy:   \n",
    "        formats[c]=lambda_acc\n",
    "    for c in col_duration:\n",
    "        formats[c]=lambda_dur\n",
    "    t=report.style.highlight_max(subset=col_accuracy).format(formats).hide_index()\n",
    "    display(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647edae-2c97-47c6-8ef3-d7c6e68fe448",
   "metadata": {},
   "source": [
    "## Step 3 - Reports display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6950b1e7-3537-4a3a-a91a-465eb46e9cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>**Report : report_nombre_neurons_2023-03-12_09h20m20s**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desc.  :  scale=0.5 batch_size=64 epochs=16 data_aug=False \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Temp\\ipykernel_10960\\2258343395.py:26: FutureWarning: this method is deprecated in favour of `Styler.hide(axis=\"index\")`\n",
      "  t=report.style.highlight_max(subset=col_accuracy).format(formats).hide_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3b27f_row0_col2, #T_3b27f_row0_col5, #T_3b27f_row0_col8, #T_3b27f_row0_col11, #T_3b27f_row0_col14, #T_3b27f_row0_col17 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3b27f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_3b27f_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
       "      <th id=\"T_3b27f_level0_col1\" class=\"col_heading level0 col1\" >Size</th>\n",
       "      <th id=\"T_3b27f_level0_col2\" class=\"col_heading level0 col2\" >create_model2_100_Accuracy</th>\n",
       "      <th id=\"T_3b27f_level0_col3\" class=\"col_heading level0 col3\" >create_model2_100_Duration</th>\n",
       "      <th id=\"T_3b27f_level0_col4\" class=\"col_heading level0 col4\" >create_model2_100_loss</th>\n",
       "      <th id=\"T_3b27f_level0_col5\" class=\"col_heading level0 col5\" >create_model2_300_Accuracy</th>\n",
       "      <th id=\"T_3b27f_level0_col6\" class=\"col_heading level0 col6\" >create_model2_300_Duration</th>\n",
       "      <th id=\"T_3b27f_level0_col7\" class=\"col_heading level0 col7\" >create_model2_300_loss</th>\n",
       "      <th id=\"T_3b27f_level0_col8\" class=\"col_heading level0 col8\" >create_model2_500_Accuracy</th>\n",
       "      <th id=\"T_3b27f_level0_col9\" class=\"col_heading level0 col9\" >create_model2_500_Duration</th>\n",
       "      <th id=\"T_3b27f_level0_col10\" class=\"col_heading level0 col10\" >create_model2_500_loss</th>\n",
       "      <th id=\"T_3b27f_level0_col11\" class=\"col_heading level0 col11\" >create_model2_700_Accuracy</th>\n",
       "      <th id=\"T_3b27f_level0_col12\" class=\"col_heading level0 col12\" >create_model2_700_Duration</th>\n",
       "      <th id=\"T_3b27f_level0_col13\" class=\"col_heading level0 col13\" >create_model2_700_loss</th>\n",
       "      <th id=\"T_3b27f_level0_col14\" class=\"col_heading level0 col14\" >create_model2_900_Accuracy</th>\n",
       "      <th id=\"T_3b27f_level0_col15\" class=\"col_heading level0 col15\" >create_model2_900_Duration</th>\n",
       "      <th id=\"T_3b27f_level0_col16\" class=\"col_heading level0 col16\" >create_model2_900_loss</th>\n",
       "      <th id=\"T_3b27f_level0_col17\" class=\"col_heading level0 col17\" >create_model2_1000_Accuracy</th>\n",
       "      <th id=\"T_3b27f_level0_col18\" class=\"col_heading level0 col18\" >create_model2_1000_Duration</th>\n",
       "      <th id=\"T_3b27f_level0_col19\" class=\"col_heading level0 col19\" >create_model2_1000_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_3b27f_row0_col0\" class=\"data row0 col0\" >img_64</td>\n",
       "      <td id=\"T_3b27f_row0_col1\" class=\"data row0 col1\" >10022.00 Mo</td>\n",
       "      <td id=\"T_3b27f_row0_col2\" class=\"data row0 col2\" >89.22 %</td>\n",
       "      <td id=\"T_3b27f_row0_col3\" class=\"data row0 col3\" >198.5 s</td>\n",
       "      <td id=\"T_3b27f_row0_col4\" class=\"data row0 col4\" >0.291375</td>\n",
       "      <td id=\"T_3b27f_row0_col5\" class=\"data row0 col5\" >88.56 %</td>\n",
       "      <td id=\"T_3b27f_row0_col6\" class=\"data row0 col6\" >208.3 s</td>\n",
       "      <td id=\"T_3b27f_row0_col7\" class=\"data row0 col7\" >0.289586</td>\n",
       "      <td id=\"T_3b27f_row0_col8\" class=\"data row0 col8\" >89.42 %</td>\n",
       "      <td id=\"T_3b27f_row0_col9\" class=\"data row0 col9\" >221.5 s</td>\n",
       "      <td id=\"T_3b27f_row0_col10\" class=\"data row0 col10\" >0.275271</td>\n",
       "      <td id=\"T_3b27f_row0_col11\" class=\"data row0 col11\" >88.76 %</td>\n",
       "      <td id=\"T_3b27f_row0_col12\" class=\"data row0 col12\" >268.1 s</td>\n",
       "      <td id=\"T_3b27f_row0_col13\" class=\"data row0 col13\" >0.307372</td>\n",
       "      <td id=\"T_3b27f_row0_col14\" class=\"data row0 col14\" >88.29 %</td>\n",
       "      <td id=\"T_3b27f_row0_col15\" class=\"data row0 col15\" >218.3 s</td>\n",
       "      <td id=\"T_3b27f_row0_col16\" class=\"data row0 col16\" >0.317400</td>\n",
       "      <td id=\"T_3b27f_row0_col17\" class=\"data row0 col17\" >90.88 %</td>\n",
       "      <td id=\"T_3b27f_row0_col18\" class=\"data row0 col18\" >233.3 s</td>\n",
       "      <td id=\"T_3b27f_row0_col19\" class=\"data row0 col19\" >0.272930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16cc6589370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for file in glob.glob(f'{report_dir}/*.json'):\n",
    "file = report_name\n",
    "show_report(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c78b8-153f-40ae-8694-c5874e6c3181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
