{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d72e6b-cde1-4088-bed5-f2322065860c",
   "metadata": {},
   "source": [
    "## Entrainement de plusieurs models et plusieurs dataset au choix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34e9d11c-2497-4f1c-8329-94443464fb67",
   "metadata": {},
   "source": [
    "il renvoie un rapport en json\n",
    "le datagen reste toujours non avant que je fais un fonction pour que le data d'image est en 4 dimension pour adapter le fonction de dataugmentation de keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a801d-6296-4268-9f81-edb9e2313752",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Python Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad95307a-c46b-4e2d-a129-665b604aad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gestion fichier modeles pythons\n",
    "import os , sys            \n",
    "workpath = sys.path[0]\n",
    "\n",
    "sys.path.append(f'{workpath}\\fidle-env\\lib\\site-packages')\n",
    "\n",
    "#modules utilitaires\n",
    "import random as r\n",
    "import numpy as np\n",
    "import time \n",
    "import json\n",
    "import fidle\n",
    "import fidle.pwk as pwk   \n",
    "#traitement image\n",
    "import matplotlib.pyplot as plt   \n",
    "from skimage import io \n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "#module IA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250092b-f23a-405f-9725-444508fcb644",
   "metadata": {},
   "source": [
    "## les diffs choix de dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b002847d-c87a-4e78-b3b4-3fc980feeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = f'{workpath}\\datasheet\\mmClasses-DataBase-IMG\\mmClasses-DataBase-IMG\\data_64'\n",
    "descri_datapath = [datapath, 'img_64']\n",
    "run_dir=\"E:\\INSA-cour\\A4A\\Projet_multi\\work-dictory\\\\trained_model\\model_64\"\n",
    "fidle.utils.mkdir(run_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead109b-f744-4b83-864a-9d29e8a339ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters for CNN and DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0c95a8e-3516-4127-b112-91ba1310b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pourcentage du dataset à load\n",
    "scale = 1  \n",
    "\n",
    "batch_size    = 64\n",
    "epochs        = 10\n",
    "fit_verbosity = 1\n",
    "\n",
    "index_to_class = [\"CUBO\",\"DEC\",\"FCC\",\"FCC-sphere\",\"HCP-sphere\",\"ICO\",\"MnBeta_sphere\",\"OH\",\"RTD\",\"BCC\",\"DODECA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118a8ed-37b1-45e8-82ee-fb8f5e552158",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3f18a92-c53e-43a1-b62e-4188e1ad0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset (filepath : str):\n",
    "    \"\"\"entrée : chemin vers le dossier contenant la BDD\n",
    "    sortie : listes des images et leur classes respectives, mis à l'échelle (scale) et mélangées\"\"\"\n",
    "    \n",
    "    L = os.listdir(filepath)   #liste contenenant le nom (en .jpg) de toutes les images\n",
    "    nb_files = len(L)\n",
    "    nb_files2load = round(nb_files*scale)\n",
    "    size = np.shape(io.imread(f'{filepath}/{L[0]}' , as_gray = True))\n",
    "    \n",
    "    #initialisation listes sortie\n",
    "    img = np.zeros ( (nb_files2load , size[0] , size[1]) , dtype = np.float16 )\n",
    "    ID  = np.zeros ( nb_files2load , dtype = np.int8)\n",
    "   \n",
    "    for i in range (nb_files2load):\n",
    "        \n",
    "        random_index = r.randint(0 , nb_files - i - 1)\n",
    "        img[i] = io.imread (f'{filepath}/{L[random_index]}',as_gray = True)\n",
    "        ID [i] = int (L[random_index][:3]) - 1\n",
    "        L.pop(random_index)\n",
    "        \n",
    "    N = len(img)\n",
    "    nb_img = N\n",
    "    img_train = img[0:round(N*0.8)]\n",
    "    img_test  = img[round(N*0.8):]\n",
    "    ID_train = ID[0:round(N*0.8)]\n",
    "    ID_test  = ID[round(N*0.8):]   \n",
    "        \n",
    "    return img_train,img_test,ID_train,ID_test,nb_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09829ea-64be-467c-a306-bee2323d1a38",
   "metadata": {},
   "source": [
    "## afficher qq images de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417c603-188c-44d5-8e6c-12cec02529cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(4):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(img_train[i], cmap =\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    ax.text(6,10,index_to_class[ID_train[i]], bbox={'facecolor': 'white', 'pad': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21a969-2e4b-43d1-b81a-e7dbccd98fe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models creations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371cfdd-3c96-4d0c-84f1-59812bdde01e",
   "metadata": {},
   "source": [
    "## comparaison des functions d'activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "859844b0-8792-4819-b52b-3e30776ca463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model1(lx,ly): \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( keras.layers.Dense(1000, activation='relu'))\n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    \n",
    "    code_model='1'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "\n",
    "def create_model1_DNNsigmod(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.sigmoid))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "              \n",
    "    code_model='1_DNNsigmod'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "              \n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "def create_model1_DNNtanh(lx,ly): \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.tanh))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNtanh'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model\n",
    "\n",
    "def create_model1_DNNsoftsign(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "              \n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.softsign))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNsoftsign'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "def create_model1_DNNelu(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.elu))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNelu'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model              \n",
    "\n",
    "\n",
    "              \n",
    "def create_model1_DNNselu(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.selu))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNselu'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model                \n",
    "              \n",
    "def create_model1_DNNsoftplus(lx,ly): #\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(1000))\n",
    "    model.add( layers.Activation(activations.softplus))          \n",
    "    model.add( keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    code_model='1_DNNsoftplus'  \n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)          \n",
    "    return model, model_name, code_model              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8355078-310d-44b0-8702-20afa442f1a1",
   "metadata": {},
   "source": [
    "## nombre de couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab24b3-0fe4-438a-9ccb-ba16c6aa2edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06856142-3d21-47a8-b713-5b8a5040db30",
   "metadata": {},
   "source": [
    "## qq variable a entrer dans les foncitons suivants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cee00af3-bac1-4ad3-a331-cd35678497c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# les entree pour le dataset 64\n",
    "datasets = [descri_datapath]\n",
    "with_datagen  = False\n",
    "tag_id = \"fonc_acti\" #si on faire un autre execution de script, n'oublie pas de changer le tag_id comme \"fonc_acti_<un date>\" ou \"nombre_de_couche\", etc.\n",
    "models = ['create_model1','create_model1_DNNsigmod','create_model1_DNNtanh','create_model1_DNNsoftsign','create_model1_DNNelu','create_model1_DNNselu','create_model1_DNNsoftplus']\n",
    "#models = ['create_model1_DNNsoftplus','create_model1_DNNselu','create_model1_DNNelu']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762c38b-b783-41bf-b6e8-45bba5c0ada4",
   "metadata": {},
   "source": [
    "## Multiple datasets, multiple models ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed889bec-a219-4311-8094-f62ca32b9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_run(datasets, models, datagen=None,\n",
    "              scale=1, batch_size=64, epochs=16, \n",
    "              fit_verbosity=0, tag_id='last'):\n",
    "    \"\"\"\n",
    "    Launches a dataset-model combination\n",
    "    args:\n",
    "        enhanced_dir   : Directory of the enhanced datasets\n",
    "        datasets       : List of dataset (whitout .h5)\n",
    "        models         : List of model like { \"model name\":get_model(), ...}\n",
    "        datagen        : Data generator or None (None)\n",
    "        scale          : % of dataset to use.  1 mean all. (1)\n",
    "        batch_size     : Batch size (64)\n",
    "        epochs         : Number of epochs (16)\n",
    "        fit_verbosity  : Verbose level (0)\n",
    "        tag_id         : postfix for report, logs and models dir (_last)\n",
    "    return:\n",
    "        report        : Report as a dict for Pandas.\n",
    "    \"\"\"  \n",
    "    # ---- Logs and models dir\n",
    "    #\n",
    "    os.makedirs(f'{run_dir}/logs_{tag_id}',   mode=0o750, exist_ok=True)\n",
    "    os.makedirs(f'{run_dir}/models_{tag_id}', mode=0o750, exist_ok=True)\n",
    "    \n",
    "    # ---- Columns of output\n",
    "    #\n",
    "    output={}\n",
    "    output['Dataset'] = []\n",
    "    output['Size']    = []\n",
    "    \n",
    "    for m in models:\n",
    "        output[m+'_Accuracy'] = []\n",
    "        output[m+'_Duration'] = []\n",
    "    \n",
    "    # ---- Let's go\n",
    "    #\n",
    "    for d_name in datasets:\n",
    "        print(\"\\nDataset : \",d_name[0])\n",
    "        \n",
    "        # ---- Read dataset\n",
    "        img_train,img_test,ID_train,ID_test, d_size = read_dataset(d_name[0])\n",
    "        d_name = d_name[1]\n",
    "        output['Dataset'].append(d_name)\n",
    "        output['Size'].append(d_size)\n",
    "        print(len(img_train))\n",
    "        print(len(ID_train))\n",
    "        # ---- Rescale c'est pas vraiment necessaire\n",
    "        ##img_train,ID_train,img_test,ID_test = pwk.rescale_dataset(img_train,ID_train,img_test,ID_test, scale=scale)\n",
    "        \n",
    "        # ---- Get the shape\n",
    "        (n,lx,ly) = img_train.shape\n",
    "\n",
    "        # ---- For each model\n",
    "        for m_function in models:\n",
    "            \n",
    "            # ---- get model\n",
    "            try:\n",
    "                # ---- get function by name\n",
    "                m = m_function\n",
    "                m_function=globals()[m_function]\n",
    "                model, m_name, code_model=m_function(lx,ly)\n",
    "                \n",
    "                print(\"    Run model {}  : \".format(code_model), end='')\n",
    "                # ---- Compile it\n",
    "                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                # ---- Callbacks tensorboard\n",
    "                log_dir = f'{run_dir}/logs_{tag_id}/tb_{d_name}_{m_name}'\n",
    "                tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                # ---- Callbacks bestmodel\n",
    "                save_dir = f'{run_dir}/models_{tag_id}/model_{d_name}_{m_name}.h5'\n",
    "                bestmodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, monitor='accuracy', save_best_only=True)\n",
    "                # ---- Train\n",
    "                start_time = time.time()\n",
    "                if datagen==None:\n",
    "                    # ---- No data augmentation (datagen=None) --------------------------------------\n",
    "                    history = model.fit(img_train, ID_train,\n",
    "                                        batch_size      = batch_size,\n",
    "                                        epochs          = epochs,\n",
    "                                        verbose         = fit_verbosity,\n",
    "                                        validation_data = (img_test, ID_test),\n",
    "                                        callbacks       = [tensorboard_callback, bestmodel_callback])\n",
    "                else:\n",
    "                    # ---- Data augmentation (datagen given) ----------------------------------------\n",
    "                    datagen.fit(x_train)\n",
    "                    history = model.fit(datagen.flow(img_train, ID_train, batch_size=batch_size),\n",
    "                                        steps_per_epoch = int(len(img_train)/batch_size),\n",
    "                                        epochs          = epochs,\n",
    "                                        verbose         = fit_verbosity,\n",
    "                                        validation_data = (img_test, ID_test),\n",
    "                                        callbacks       = [tensorboard_callback, bestmodel_callback])\n",
    "                    \n",
    "                # ---- Result\n",
    "                end_time = time.time()\n",
    "                duration = end_time-start_time\n",
    "                accuracy = max(history.history[\"val_accuracy\"])*100\n",
    "                #\n",
    "                output[m+'_Accuracy'].append(accuracy)\n",
    "                #output[m+'_Accuracy'] = accuracy\n",
    "                output[m+'_Duration'].append(duration)\n",
    "                #output[m+'_Duration'] = duration\n",
    "                print(f\"Accuracy={accuracy: 7.2f}    Duration={duration: 7.2f}\")\n",
    "            except:\n",
    "                raise\n",
    "                output[m+'_Accuracy'].append('0')\n",
    "                output[m+'_Duration'].append('999')\n",
    "                print('-')\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b497c82-e1b9-49cd-89e8-eac20d57eabb",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37c63152-81e5-4563-83ca-956d8b9687b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Run --------------------------------------------------\n",
      "\n",
      "Dataset :  E:\\INSA-cour\\A4A\\Projet_multi\\work-dictory\\datasheet\\mmClasses-DataBase-IMG\\mmClasses-DataBase-IMG\\data_64\n",
      "16032\n",
      "16032\n",
      "    Run model 1  : Epoch 1/10\n",
      "251/251 [==============================] - 32s 125ms/step - loss: 1.8766 - accuracy: 0.2960 - val_loss: 1.2366 - val_accuracy: 0.5277\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 1.0759 - accuracy: 0.5679 - val_loss: 0.7612 - val_accuracy: 0.7018\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 30s 118ms/step - loss: 0.7546 - accuracy: 0.6950 - val_loss: 0.6840 - val_accuracy: 0.7161\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 28s 113ms/step - loss: 0.6008 - accuracy: 0.7607 - val_loss: 0.5279 - val_accuracy: 0.7884\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 29s 115ms/step - loss: 0.4906 - accuracy: 0.8051 - val_loss: 0.4118 - val_accuracy: 0.8276\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 0.4103 - accuracy: 0.8394 - val_loss: 0.2668 - val_accuracy: 0.9124\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 27s 109ms/step - loss: 0.3472 - accuracy: 0.8635 - val_loss: 0.2361 - val_accuracy: 0.9154\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 28s 111ms/step - loss: 0.2957 - accuracy: 0.8845 - val_loss: 0.2079 - val_accuracy: 0.9291\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 27s 109ms/step - loss: 0.2599 - accuracy: 0.8987 - val_loss: 0.1490 - val_accuracy: 0.9436\n",
      "Epoch 10/10\n",
      "251/251 [==============================] - 28s 112ms/step - loss: 0.2419 - accuracy: 0.9053 - val_loss: 0.1604 - val_accuracy: 0.9361\n",
      "Accuracy=  94.36    Duration= 287.48\n",
      "    Run model 1_DNNsigmod  : Epoch 1/10\n",
      "251/251 [==============================] - 31s 122ms/step - loss: 2.1049 - accuracy: 0.2320 - val_loss: 1.5851 - val_accuracy: 0.3678\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 29s 115ms/step - loss: 1.3209 - accuracy: 0.4759 - val_loss: 0.9715 - val_accuracy: 0.6053\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 28s 112ms/step - loss: 0.9715 - accuracy: 0.6018 - val_loss: 0.7039 - val_accuracy: 0.7250\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 29s 115ms/step - loss: 0.7517 - accuracy: 0.7005 - val_loss: 0.6797 - val_accuracy: 0.7345\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 29s 115ms/step - loss: 0.6225 - accuracy: 0.7502 - val_loss: 0.4756 - val_accuracy: 0.8184\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 29s 116ms/step - loss: 0.5198 - accuracy: 0.7931 - val_loss: 0.3504 - val_accuracy: 0.8703\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 29s 116ms/step - loss: 0.4574 - accuracy: 0.8149 - val_loss: 0.3120 - val_accuracy: 0.8802\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 29s 114ms/step - loss: 0.3866 - accuracy: 0.8454 - val_loss: 0.3092 - val_accuracy: 0.8812\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 29s 115ms/step - loss: 0.3436 - accuracy: 0.8643 - val_loss: 0.2383 - val_accuracy: 0.9059\n",
      "Epoch 10/10\n",
      "251/251 [==============================] - 29s 115ms/step - loss: 0.3139 - accuracy: 0.8789 - val_loss: 0.2963 - val_accuracy: 0.8907\n",
      "Accuracy=  90.59    Duration= 290.42\n",
      "    Run model 1_DNNtanh  : Epoch 1/10\n",
      "251/251 [==============================] - 32s 125ms/step - loss: 1.7814 - accuracy: 0.3230 - val_loss: 1.0887 - val_accuracy: 0.5504\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 29s 117ms/step - loss: 1.0138 - accuracy: 0.5961 - val_loss: 0.7766 - val_accuracy: 0.6874\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.7349 - accuracy: 0.7055 - val_loss: 0.5053 - val_accuracy: 0.8186\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 29s 117ms/step - loss: 0.5816 - accuracy: 0.7716 - val_loss: 0.4672 - val_accuracy: 0.8276\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 29s 117ms/step - loss: 0.4897 - accuracy: 0.8074 - val_loss: 0.3193 - val_accuracy: 0.8807\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.4053 - accuracy: 0.8435 - val_loss: 0.3488 - val_accuracy: 0.8625\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.3479 - accuracy: 0.8648 - val_loss: 0.2162 - val_accuracy: 0.9244\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 30s 118ms/step - loss: 0.3049 - accuracy: 0.8835 - val_loss: 0.1870 - val_accuracy: 0.9289\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 29s 117ms/step - loss: 0.2652 - accuracy: 0.8986 - val_loss: 0.1679 - val_accuracy: 0.9409\n",
      "Epoch 10/10\n",
      "251/251 [==============================] - 29s 117ms/step - loss: 0.2519 - accuracy: 0.9044 - val_loss: 0.1504 - val_accuracy: 0.9446\n",
      "Accuracy=  94.46    Duration= 298.80\n",
      "    Run model 1_DNNsoftsign  : Epoch 1/10\n",
      "251/251 [==============================] - 32s 126ms/step - loss: 1.8764 - accuracy: 0.2970 - val_loss: 1.2461 - val_accuracy: 0.5202\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 1.0591 - accuracy: 0.5689 - val_loss: 0.7289 - val_accuracy: 0.7206\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.7539 - accuracy: 0.6945 - val_loss: 0.6738 - val_accuracy: 0.7308\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.6081 - accuracy: 0.7546 - val_loss: 0.4516 - val_accuracy: 0.8164\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 30s 118ms/step - loss: 0.5000 - accuracy: 0.8016 - val_loss: 0.3353 - val_accuracy: 0.8695\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.4287 - accuracy: 0.8313 - val_loss: 0.2875 - val_accuracy: 0.8912\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 30s 118ms/step - loss: 0.3625 - accuracy: 0.8612 - val_loss: 0.2532 - val_accuracy: 0.9037\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.3330 - accuracy: 0.8693 - val_loss: 0.2131 - val_accuracy: 0.9234\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.2818 - accuracy: 0.8925 - val_loss: 0.1557 - val_accuracy: 0.9476\n",
      "Epoch 10/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.2515 - accuracy: 0.9051 - val_loss: 0.2102 - val_accuracy: 0.9199\n",
      "Accuracy=  94.76    Duration= 301.45\n",
      "    Run model 1_DNNelu  : Epoch 1/10\n",
      "251/251 [==============================] - 34s 133ms/step - loss: 1.8435 - accuracy: 0.3053 - val_loss: 1.3104 - val_accuracy: 0.4678\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 1.0283 - accuracy: 0.5873 - val_loss: 0.7947 - val_accuracy: 0.6704\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 31s 122ms/step - loss: 0.7683 - accuracy: 0.6914 - val_loss: 0.6884 - val_accuracy: 0.7061\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 30s 121ms/step - loss: 0.6301 - accuracy: 0.7479 - val_loss: 0.4495 - val_accuracy: 0.8383\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.5223 - accuracy: 0.7967 - val_loss: 0.2994 - val_accuracy: 0.8920\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.4519 - accuracy: 0.8236 - val_loss: 0.3066 - val_accuracy: 0.8890\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.3992 - accuracy: 0.8456 - val_loss: 0.2485 - val_accuracy: 0.9114\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 31s 123ms/step - loss: 0.3466 - accuracy: 0.8662 - val_loss: 0.2359 - val_accuracy: 0.9102\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.3226 - accuracy: 0.8774 - val_loss: 0.2550 - val_accuracy: 0.9014\n",
      "Epoch 10/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.2920 - accuracy: 0.8914 - val_loss: 0.1988 - val_accuracy: 0.9242\n",
      "Accuracy=  92.42    Duration= 306.81\n",
      "    Run model 1_DNNselu  : Epoch 1/10\n",
      "251/251 [==============================] - 31s 121ms/step - loss: 1.8239 - accuracy: 0.3078 - val_loss: 1.1750 - val_accuracy: 0.5250\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 1.0249 - accuracy: 0.5833 - val_loss: 0.7278 - val_accuracy: 0.7263\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.7357 - accuracy: 0.7086 - val_loss: 0.5737 - val_accuracy: 0.7812\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.6004 - accuracy: 0.7599 - val_loss: 0.3635 - val_accuracy: 0.8655\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.4726 - accuracy: 0.8154 - val_loss: 0.3105 - val_accuracy: 0.8847\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 30s 118ms/step - loss: 0.4214 - accuracy: 0.8369 - val_loss: 0.2676 - val_accuracy: 0.9037\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.3578 - accuracy: 0.8641 - val_loss: 0.2310 - val_accuracy: 0.9232\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 30s 118ms/step - loss: 0.3124 - accuracy: 0.8812 - val_loss: 0.2368 - val_accuracy: 0.9159\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.2873 - accuracy: 0.8922 - val_loss: 0.3000 - val_accuracy: 0.8827\n",
      "Epoch 10/10\n",
      "251/251 [==============================] - 30s 118ms/step - loss: 0.2578 - accuracy: 0.9013 - val_loss: 0.1538 - val_accuracy: 0.9416\n",
      "Accuracy=  94.16    Duration= 300.26\n",
      "    Run model 1_DNNsoftplus  : Epoch 1/10\n",
      "251/251 [==============================] - 31s 121ms/step - loss: 2.0903 - accuracy: 0.2355 - val_loss: 1.7323 - val_accuracy: 0.3386\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 1.3200 - accuracy: 0.4724 - val_loss: 1.2192 - val_accuracy: 0.5240\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.9410 - accuracy: 0.6150 - val_loss: 0.7101 - val_accuracy: 0.6944\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.7343 - accuracy: 0.6996 - val_loss: 0.5211 - val_accuracy: 0.7992\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.5985 - accuracy: 0.7552 - val_loss: 0.4448 - val_accuracy: 0.8191\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.5232 - accuracy: 0.7915 - val_loss: 0.3590 - val_accuracy: 0.8545\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.4415 - accuracy: 0.8212 - val_loss: 0.2463 - val_accuracy: 0.9127\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.3761 - accuracy: 0.8502 - val_loss: 0.2366 - val_accuracy: 0.9054\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 30s 119ms/step - loss: 0.3329 - accuracy: 0.8656 - val_loss: 0.3225 - val_accuracy: 0.8613\n",
      "Epoch 10/10\n",
      "251/251 [==============================] - 30s 120ms/step - loss: 0.3039 - accuracy: 0.8774 - val_loss: 0.1804 - val_accuracy: 0.9336\n",
      "Accuracy=  93.36    Duration= 300.71\n",
      "\n",
      "Report saved as  E:\\INSA-cour\\A4A\\Projet_multi\\work-dictory\\trained_model\\model_64/report_fonc_acti.json\n",
      "\n",
      "Duration :  00:36:02 823ms\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pwk.chrono_start()\n",
    "\n",
    "print('\\n---- Run','-'*50)\n",
    "\n",
    "\n",
    "# ---- Data augmentation or not\n",
    "#\n",
    "if with_datagen :\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n",
    "                                                           featurewise_std_normalization=False,\n",
    "                                                           width_shift_range=0.1,\n",
    "                                                           height_shift_range=0.1,\n",
    "                                                           zoom_range=0.2,\n",
    "                                                           shear_range=0.1,\n",
    "                                                           rotation_range=10.)\n",
    "else:\n",
    "    datagen=None\n",
    "    \n",
    "# ---- Run\n",
    "#\n",
    "output = multi_run(datasets, \n",
    "                   models,\n",
    "                   datagen       = datagen,\n",
    "                   scale         = scale,\n",
    "                   batch_size    = batch_size,\n",
    "                   epochs        = epochs,\n",
    "                   fit_verbosity = fit_verbosity,\n",
    "                   tag_id        = tag_id)\n",
    "\n",
    "# ---- Save report\n",
    "#\n",
    "report={}\n",
    "report['output']=output\n",
    "report['description'] = f'scale={scale} batch_size={batch_size} epochs={epochs} data_aug={with_datagen}'\n",
    "\n",
    "report_name=f'{run_dir}/report_{tag_id}.json'\n",
    "\n",
    "with open(report_name, 'w') as file:\n",
    "    json.dump(report, file, indent=4)\n",
    "\n",
    "print('\\nReport saved as ',report_name)\n",
    "\n",
    "pwk.chrono_show()\n",
    "print('-'*59)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582c42f-798c-4ee4-b422-e4202e28b268",
   "metadata": {},
   "source": [
    "## show report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d5807c0-9280-45e6-98b9-c83193db1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dir = run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "097f6b7f-873e-4844-82e9-790cd6a3c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = (s == s.max())\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "def show_report(file):\n",
    "    # ---- Read json file\n",
    "    with open(file) as infile:\n",
    "        dict_report = json.load( infile )\n",
    "    output      = dict_report['output']\n",
    "    description = dict_report['description']\n",
    "    # ---- about\n",
    "    pwk.subtitle(f'Report : {Path(file).stem}')\n",
    "    print(    \"Desc.  : \",description,'\\n')\n",
    "    # ---- Create a pandas\n",
    "    report       = pd.DataFrame (output)\n",
    "    col_accuracy = [ c for c in output.keys() if c.endswith('Accuracy')]\n",
    "    col_duration = [ c for c in output.keys() if c.endswith('Duration')]\n",
    "    # ---- Build formats\n",
    "    lambda_acc = lambda x : '{:.2f} %'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n",
    "    lambda_dur = lambda x : '{:.1f} s'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n",
    "    formats = {'Size':'{:.2f} Mo'}\n",
    "    for c in col_accuracy:   \n",
    "        formats[c]=lambda_acc\n",
    "    for c in col_duration:\n",
    "        formats[c]=lambda_dur\n",
    "    t=report.style.highlight_max(subset=col_accuracy).format(formats).hide_index()\n",
    "    display(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647edae-2c97-47c6-8ef3-d7c6e68fe448",
   "metadata": {},
   "source": [
    "## Step 3 - Reports display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6950b1e7-3537-4a3a-a91a-465eb46e9cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>**Report : report_fonc_acti**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desc.  :  scale=1 batch_size=64 epochs=10 data_aug=False \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Temp\\ipykernel_9932\\2258343395.py:26: FutureWarning: this method is deprecated in favour of `Styler.hide(axis=\"index\")`\n",
      "  t=report.style.highlight_max(subset=col_accuracy).format(formats).hide_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a634d_row0_col2, #T_a634d_row0_col4, #T_a634d_row0_col6, #T_a634d_row0_col8, #T_a634d_row0_col10, #T_a634d_row0_col12, #T_a634d_row0_col14 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a634d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_a634d_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
       "      <th id=\"T_a634d_level0_col1\" class=\"col_heading level0 col1\" >Size</th>\n",
       "      <th id=\"T_a634d_level0_col2\" class=\"col_heading level0 col2\" >create_model1_Accuracy</th>\n",
       "      <th id=\"T_a634d_level0_col3\" class=\"col_heading level0 col3\" >create_model1_Duration</th>\n",
       "      <th id=\"T_a634d_level0_col4\" class=\"col_heading level0 col4\" >create_model1_DNNsigmod_Accuracy</th>\n",
       "      <th id=\"T_a634d_level0_col5\" class=\"col_heading level0 col5\" >create_model1_DNNsigmod_Duration</th>\n",
       "      <th id=\"T_a634d_level0_col6\" class=\"col_heading level0 col6\" >create_model1_DNNtanh_Accuracy</th>\n",
       "      <th id=\"T_a634d_level0_col7\" class=\"col_heading level0 col7\" >create_model1_DNNtanh_Duration</th>\n",
       "      <th id=\"T_a634d_level0_col8\" class=\"col_heading level0 col8\" >create_model1_DNNsoftsign_Accuracy</th>\n",
       "      <th id=\"T_a634d_level0_col9\" class=\"col_heading level0 col9\" >create_model1_DNNsoftsign_Duration</th>\n",
       "      <th id=\"T_a634d_level0_col10\" class=\"col_heading level0 col10\" >create_model1_DNNelu_Accuracy</th>\n",
       "      <th id=\"T_a634d_level0_col11\" class=\"col_heading level0 col11\" >create_model1_DNNelu_Duration</th>\n",
       "      <th id=\"T_a634d_level0_col12\" class=\"col_heading level0 col12\" >create_model1_DNNselu_Accuracy</th>\n",
       "      <th id=\"T_a634d_level0_col13\" class=\"col_heading level0 col13\" >create_model1_DNNselu_Duration</th>\n",
       "      <th id=\"T_a634d_level0_col14\" class=\"col_heading level0 col14\" >create_model1_DNNsoftplus_Accuracy</th>\n",
       "      <th id=\"T_a634d_level0_col15\" class=\"col_heading level0 col15\" >create_model1_DNNsoftplus_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_a634d_row0_col0\" class=\"data row0 col0\" >img_64</td>\n",
       "      <td id=\"T_a634d_row0_col1\" class=\"data row0 col1\" >20040.00 Mo</td>\n",
       "      <td id=\"T_a634d_row0_col2\" class=\"data row0 col2\" >94.36 %</td>\n",
       "      <td id=\"T_a634d_row0_col3\" class=\"data row0 col3\" >287.5 s</td>\n",
       "      <td id=\"T_a634d_row0_col4\" class=\"data row0 col4\" >90.59 %</td>\n",
       "      <td id=\"T_a634d_row0_col5\" class=\"data row0 col5\" >290.4 s</td>\n",
       "      <td id=\"T_a634d_row0_col6\" class=\"data row0 col6\" >94.46 %</td>\n",
       "      <td id=\"T_a634d_row0_col7\" class=\"data row0 col7\" >298.8 s</td>\n",
       "      <td id=\"T_a634d_row0_col8\" class=\"data row0 col8\" >94.76 %</td>\n",
       "      <td id=\"T_a634d_row0_col9\" class=\"data row0 col9\" >301.5 s</td>\n",
       "      <td id=\"T_a634d_row0_col10\" class=\"data row0 col10\" >92.42 %</td>\n",
       "      <td id=\"T_a634d_row0_col11\" class=\"data row0 col11\" >306.8 s</td>\n",
       "      <td id=\"T_a634d_row0_col12\" class=\"data row0 col12\" >94.16 %</td>\n",
       "      <td id=\"T_a634d_row0_col13\" class=\"data row0 col13\" >300.3 s</td>\n",
       "      <td id=\"T_a634d_row0_col14\" class=\"data row0 col14\" >93.36 %</td>\n",
       "      <td id=\"T_a634d_row0_col15\" class=\"data row0 col15\" >300.7 s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x151c4eff820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file in glob.glob(f'{report_dir}/*.json'):\n",
    "    show_report(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c78b8-153f-40ae-8694-c5874e6c3181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
