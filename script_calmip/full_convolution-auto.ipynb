{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d72e6b-cde1-4088-bed5-f2322065860c",
   "metadata": {},
   "source": [
    "## Entrainement de plusieurs models et plusieurs dataset au choix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34e9d11c-2497-4f1c-8329-94443464fb67",
   "metadata": {},
   "source": [
    "###il renvoie un rapport en json\n",
    "###le datagen reste toujours \"non\" avant que je fais un fonction pour que le data d'image est en 4 dimension pour adapter le fonction de dataugmentation de keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a801d-6296-4268-9f81-edb9e2313752",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Python Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95307a-c46b-4e2d-a129-665b604aad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gestion fichier modeles pythons\n",
    "import os , sys\n",
    "from os import path\n",
    "\n",
    "#modules utilitaires\n",
    "import random as r\n",
    "import numpy as np\n",
    "import time \n",
    "import json\n",
    "import pwk   \n",
    "#traitement image\n",
    "import matplotlib.pyplot as plt   \n",
    "import skimage as sm\n",
    "from skimage import io \n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import bdd_prs as bdd\n",
    "\n",
    "#module IA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef02fb-d3d4-468a-9b7c-c4e276bbfb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vérification d'existance de path, pas nécessaire à executer\n",
    "workpath = sys.path[0]\n",
    "mon_repertoire = f'{workpath}/..'\n",
    "path_exit = path.isdir(mon_repertoire)\n",
    "print(f\"path mon_repertoire existe? {path_exit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5a209-d1cf-4505-971a-ec40865bad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250092b-f23a-405f-9725-444508fcb644",
   "metadata": {},
   "source": [
    "## les diffs choix de dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002847d-c87a-4e78-b3b4-3fc980feeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = f\"{mon_repertoire}/data_128_equilibre\"\n",
    "descri_datapath = [datapath, 'img_128']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3aed91-c2ec-46ae-af5a-83842765904f",
   "metadata": {},
   "source": [
    "## les path à stocker les models entrainées, leur fichiers de tensorborad et de json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee8bc6-76e0-4d79-b529-e7396e4c52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir=f\"{mon_repertoire}/trained_model_img_models/model_128_equilibre\"\n",
    "os.makedirs(run_dir, mode=0o700, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead109b-f744-4b83-864a-9d29e8a339ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters for CNN and DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c95a8e-3516-4127-b112-91ba1310b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pourcentage du dataset à load\n",
    "scale = 1\n",
    "\n",
    "batch_size    = 64\n",
    "epochs        = 15\n",
    "fit_verbosity = 1\n",
    "\n",
    "index_to_class = [\"CUBO\",\"DEC\",\"FCC\",\"FCC-sphere\",\"HCP-sphere\",\"ICO\",\"MnBeta_sphere\",\"OH\",\"RTD\",\"BCC\",\"DODECA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118a8ed-37b1-45e8-82ee-fb8f5e552158",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30219b9e-4951-493f-a8f6-10ca6f2cbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset (filepath : str):\n",
    "    \"\"\"entrée : chemin vers le dossier contenant la BDD\n",
    "    sortie : listes des images et leur classes respectives, mis à l'échelle (scale) et mélangées\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    au lieu d'avoir un set de training 80%, un set de test 20%, j'ai appliqué ici un strategie conseillé si on a beacoup de donnee, \n",
    "    un training set 70%, un test set pour validation 15%, et un test final set 15% pour faire l'evaluation. Vu que le test set a une effet parasit sur le training set\n",
    "    \"\"\"\n",
    "    \n",
    "    L = os.listdir(filepath)   #liste contenenant le nom (en .jpg) de toutes les images\n",
    "    nb_files = len(L)\n",
    "    nb_files2load = round(nb_files*scale)\n",
    "    size = np.shape(io.imread(f'{filepath}/{L[0]}' , as_gray = True))\n",
    "    \n",
    "    #initialisation listes sortie\n",
    "    img = np.zeros ( (nb_files2load , size[0] , size[1]) , dtype = np.float16 )\n",
    "    ID  = np.zeros ( nb_files2load , dtype = np.int8)\n",
    "   \n",
    "    for i in range (nb_files2load):\n",
    "        \n",
    "        random_index = np.random.randint(0 , nb_files - i )\n",
    "        img[i] = io.imread (f'{filepath}/{L[random_index]}',as_gray = True)\n",
    "        ID [i] = int (L[random_index][:3]) - 1\n",
    "        L.pop(random_index)    \n",
    "        \n",
    "    N = len(img)\n",
    "    nb_img = N\n",
    "    a = N*0.7\n",
    "    b = N*0.85\n",
    "    img_train = img[0:round(a)]\n",
    "    img_test  = img[round(a):round(b)]\n",
    "    img_test_final = img[round(b):]\n",
    "    ID_train = ID[0:round(a)]\n",
    "    ID_test  = ID[round(a):round(b)]  \n",
    "    ID_test_final = ID[round(b):]\n",
    "    \n",
    "        \n",
    "    return img_train,img_test,ID_train,ID_test,nb_img, img_test_final, ID_test_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461bdaa-550d-4865-823a-33fd133b9c62",
   "metadata": {},
   "source": [
    "## dataugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517a015-fcc7-4240-906c-ce02f043bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataugmentation(ID : dict , img_train , ID_train): \n",
    "    \"\"\"\n",
    "    ID est les classes et les taux d'augmentation respective \n",
    "    \"\"\"\n",
    "    for Id in ID: # pour tester les entrees sont bonne\n",
    "        assert 0 <= Id <= 10, (\"le classe doit etre compris entre 0 et 10\")\n",
    "        assert 0 <= ID[Id] <= 1, (\"le taux d'augmentation doit etre compris entre 0 et 1\")  \n",
    "    L = len(img_train)\n",
    "    L1 = len(ID_train)\n",
    "    assert L == L1, (\"array image et son ID doit avoir la meme longeur\")\n",
    "    #balayer tout les images\n",
    "    for i in range(L-1):\n",
    "        y = ID_train[i]\n",
    "        img = img_train[i]\n",
    "        #cette image correspond-t-il a un classe on veux dataugmenter? \n",
    "        for Id in ID:\n",
    "            if y == Id:\n",
    "                img_ro = bdd.light_modif(img)\n",
    "                \n",
    "                #insertion d'image creer et son ID dans un position random \n",
    "                random_index = np.random.randint(0,L)\n",
    "                img_train = np.insert(img_train, random_index, img_ro, axis = 0)\n",
    "                ID_train = np.insert(ID_train, random_index, Id, axis = 0) \n",
    "\n",
    "    print(f'longeur actuelle est {img_train.shape[0]}, longeur initial est {L}')            \n",
    "    return img_train, ID_train\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21a969-2e4b-43d1-b81a-e7dbccd98fe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models creations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f46555-7bd0-41a8-8d6d-abe99ff56f28",
   "metadata": {},
   "source": [
    "## pour tester la reprodubilite de resultat de cnn (tu peux mettre qq meme structure de cnn mais les nomme differament)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ecedb-d722-4fa1-8594-8dc298612e43",
   "metadata": {},
   "source": [
    "## les models ou on convolue un truc plusieur fois devant un maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bddc3ed-11d9-4968-8e4f-fa0776950eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2_700_112234(lx,ly): #32:1, 64:2, 128:3, 256:4, ailleur, le padding me permet d'avoir le couche 256\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3), padding = 'same',  activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.Conv2D(32, (3, 3), padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3),padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(256, (3, 3),padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(700))\n",
    "    model.add( layers.Dense(700))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_700_112234'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "def m2_700_11234(lx,ly): #32:1, 64:2, 128:3, 256:4\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(32, (3,3), padding = 'same',  activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.Conv2D(32, (3, 3), padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3),padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(256, (3, 3),padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(700))\n",
    "    model.add( layers.Dense(700))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_700_11234'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model\n",
    "\n",
    "\n",
    "def m_ince_11234(lx,ly): # _ince: en applicant les kernel 1*1\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(16, (1,1), padding = 'same',  activation='relu', input_shape=(lx,ly,1)))\n",
    "    model.add( keras.layers.Conv2D(32, (3, 3), padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.Conv2D(32, (3, 3), padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((3, 3)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3),padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "    \n",
    "    model.add( keras.layers.Conv2D(256, (3, 3),padding = 'same', activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( layers.Dense(700))\n",
    "    model.add( layers.Dense(700))\n",
    "    model.add( layers.Activation(activations.tanh)) \n",
    "    model.add( keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add( keras.layers.Dense(11, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    code_model='2_700_11234'\n",
    "    model_name = 'e'+str(epochs)+'bs'+str(batch_size)+'sc'+str(scale)+'code'+str(code_model)\n",
    "    return model, model_name, code_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06856142-3d21-47a8-b713-5b8a5040db30",
   "metadata": {},
   "source": [
    "## qq variable a entrer dans les foncitons suivants (n'oublier pas de changer tag_id si on change de modele, dataset, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee00af3-bac1-4ad3-a331-cd35678497c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# les entree pour le dataset 64\n",
    "datasets = [descri_datapath]\n",
    "with_datagen  = [False] #pour diviser l'entrainement en 2 cas: avec et sans dataugmentation, si tu ne veux qu'un cas, supprimes l'element que tu veux pas\n",
    "t = pwk.tag_now()\n",
    "tag_id = f'cnn_fVGG_{str(t)}' #si on faire un autre execution de script, n'oublie pas de changer le tag_id comme \"fonc_acti_<un date>\" ou \"nombre_de_couche\", etc.\n",
    "#tag_id = 'optimizer_08_03_v2'\n",
    "#en fait, on peut lance plusieurs fois les meme entrainements, pour surperposer les courbes sur le meme schema dans tensorboard.\n",
    "#models = ['create_model1','create_model1_DNNsigmod','create_model1_DNNtanh','create_model1_DNNsoftsign','create_model1_DNNelu','create_model1_DNNselu','create_model1_DNNsoftplus']\n",
    "#models = ['create_model1_100','create_model2_100','create_model3_100']\n",
    "#models = ['create_model1_DNNtanh_adam','create_model1_DNNtanh_Adamax','create_model1_DNNtanh_Nadam','create_model1_DNNtanh_RMSprop']\n",
    "#models = ['create_model2_100','create_model2_300','create_model2_500','create_model2_700','create_model2_900','create_model2_1000']\n",
    "#models = ['m2_2000_dr_01','m2_2000_dr_02','m2_2000_dr_03','m2_2000_dr_04','m2_2000_dr_05']\n",
    "#models  = ['m2_700_pad_4','m2_700_pad_4_1','m2_700_pad_4_2','m2_700_pad_4_3']\n",
    "models = ['m2_700_112234']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3165b-6ee1-4d85-b7c7-c4d9c8200c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_bad = {4 : 0.2} # parametre de dataugmentation. Le premier est la ID de la classe, le 2eme est le taux de production de nouveaux image de ce ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762c38b-b783-41bf-b6e8-45bba5c0ada4",
   "metadata": {},
   "source": [
    "## Multiple datasets, multiple models ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed889bec-a219-4311-8094-f62ca32b9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_run(datasets, models, with_datagen = [False],\n",
    "              scale=1, batch_size=64, epochs=16, \n",
    "              fit_verbosity=0, tag_id='last'):\n",
    "    \"\"\"\n",
    "    Launches a dataset-model combination\n",
    "    args:\n",
    "        enhanced_dir   : Directory of the enhanced datasets\n",
    "        datasets       : List of dataset (whitout .h5)\n",
    "        models         : List of model like { \"model name\":get_model(), ...}\n",
    "        datagen        : Data generator or None (None)\n",
    "        scale          : % of dataset to use.  1 mean all. (1)\n",
    "        batch_size     : Batch size (64)\n",
    "        epochs         : Number of epochs (16)\n",
    "        fit_verbosity  : Verbose level (0)\n",
    "        tag_id         : postfix for report, logs and models dir (_last)\n",
    "    return:\n",
    "        report        : Report as a dict for Pandas.\n",
    "    \"\"\"  \n",
    "    # ---- Logs and models dir\n",
    "    #\n",
    "    os.makedirs(f'{run_dir}/logs_{tag_id}',   mode=0o750, exist_ok=True)\n",
    "    os.makedirs(f'{run_dir}/models_{tag_id}', mode=0o750, exist_ok=True)\n",
    "    \n",
    "    # ---- Columns of output\n",
    "    #\n",
    "    output={}\n",
    "    output['Dataset'] = []\n",
    "    output['Size']    = []\n",
    "    \n",
    "    for m in models:\n",
    "        output[m+'_sdg_Accuracy'] = []\n",
    "        output[m+'_sdg_Duration'] = []\n",
    "        output[m+'_sdg_loss'] = []\n",
    "        output[m+'_wdg_Accuracy'] = []\n",
    "        output[m+'_wdg_Duration'] = []\n",
    "        output[m+'_wdg_loss'] = []\n",
    "    # ---- Let's go\n",
    "    #\n",
    "    for d_name in datasets:\n",
    "        print(\"\\nDataset : \",d_name[0])\n",
    "        \n",
    "        # ---- Read dataset\n",
    "        img_train,img_test,ID_train,ID_test, d_size, img_test_final, ID_test_final = read_dataset(d_name[0])\n",
    "        imgs = [img_train,img_test,ID_train,ID_test, img_test_final, ID_test_final]\n",
    "        d_name = d_name[1]\n",
    "        output['Dataset'].append(d_name)\n",
    "        output['Size'].append(d_size)\n",
    "        print(len(img_train))\n",
    "        print(len(ID_train))\n",
    "        # ---- Rescale c'est pas vraiment necessaire\n",
    "        ##img_train,ID_train,img_test,ID_test = pwk.rescale_dataset(img_train,ID_train,img_test,ID_test, scale=scale)\n",
    "        \n",
    "        # ---- Get the shape\n",
    "        (n,lx,ly) = img_train.shape\n",
    "\n",
    "        # ---- For each model\n",
    "        for m_function in models:\n",
    "            \n",
    "            # ---- get model\n",
    "            try:\n",
    "                # ---- get function by name\n",
    "                m = m_function\n",
    "                m_function=globals()[m_function]\n",
    "                model_wdg, m_name, code_model=m_function(lx,ly)\n",
    "                model_sdg = m_function(lx,ly)[0]\n",
    "                print(\"    Run model {}  : \".format(code_model), end='')\n",
    "                # ---- Compile it # si on ne compare pas de diffs optimizer on ajoute ce ligne.\n",
    "                if tag_id[:6] != 'opitmizer':\n",
    "                    model_wdg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                    model_sdg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                # ---- Callbacks tensorboard\n",
    "                #_wdg == avec dataugmentation, _sdg == sans dataugmentation\n",
    "                log_dir_wdg = f'{run_dir}/logs_{tag_id}/tb_wdg_{d_name}_{m_name}'\n",
    "                tensorboard_callback_wdg = tf.keras.callbacks.TensorBoard(log_dir=log_dir_wdg, histogram_freq=1)\n",
    "                \n",
    "                log_dir_sdg = f'{run_dir}/logs_{tag_id}/tb_sdg_{d_name}_{m_name}'\n",
    "                tensorboard_callback_sdg = tf.keras.callbacks.TensorBoard(log_dir=log_dir_sdg, histogram_freq=1)\n",
    "                # ---- Callbacks bestmodel\n",
    "                save_dir_wdg = f'{run_dir}/models_{tag_id}/model_wdg_{d_name}_{m_name}.h5'\n",
    "                bestmodel_callback_wdg = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir_wdg, verbose=0, monitor='accuracy', save_best_only=True)\n",
    "                \n",
    "                save_dir_sdg = f'{run_dir}/models_{tag_id}/model_sdg_{d_name}_{m_name}.h5'\n",
    "                bestmodel_callback_sdg = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir_sdg, verbose=0, monitor='accuracy', save_best_only=True)\n",
    "                # ---- Train\n",
    "                \n",
    "                for datagen in with_datagen:\n",
    "                    start_time = time.time()\n",
    "                    # ---- No data augmentation (datagen=None) --------------------------------------\n",
    "                    if datagen==False:\n",
    "                        print(\"sans dataugmentation\")\n",
    "                        history = model_sdg.fit(img_train, ID_train,\n",
    "                                            batch_size      = batch_size,\n",
    "                                            epochs          = epochs,\n",
    "                                            verbose         = fit_verbosity,\n",
    "                                            validation_data = (img_test, ID_test),\n",
    "                                            callbacks       = [tensorboard_callback_sdg, bestmodel_callback_sdg])\n",
    "                        end_time = time.time()\n",
    "                        duration = end_time-start_time\n",
    "                        loss, accuracy = model_sdg.evaluate(img_test_final, ID_test_final, batch_size=batch_size)\n",
    "                        #resultat pour fichier json\n",
    "                        output[m+'_sdg_Accuracy'].append(accuracy)\n",
    "                        output[m+'_sdg_Duration'].append(duration)\n",
    "                        output[m+'_sdg_loss'].append(loss)\n",
    "                        print(f\"Accuracy={accuracy: 7.2f}    Duration={duration: 7.2f}\")\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"with dataugmentation\")\n",
    "                         # ---- Data augmentation (datagen given) ----------------------------------------\n",
    "                            \n",
    "                        img_train_wdg, ID_train_wdg = dataugmentation(ID = ID_bad , img_train = img_train , ID_train = ID_train)\n",
    "                        history = model_wdg.fit(img_train_wdg, ID_train_wdg,\n",
    "                                            batch_size      = batch_size,\n",
    "                                            epochs          = epochs,\n",
    "                                            verbose         = fit_verbosity,\n",
    "                                            validation_data = (img_test, ID_test),\n",
    "                                            callbacks       = [tensorboard_callback_wdg, bestmodel_callback_wdg])\n",
    "                        end_time = time.time()\n",
    "                        duration = end_time-start_time\n",
    "                        loss, accuracy = model_wdg.evaluate(img_test_final, ID_test_final, batch_size=batch_size)\n",
    "                        #resultats pour fichier json \n",
    "                        output[m+'_wdg_Accuracy'].append(accuracy)\n",
    "                        output[m+'_wdg_Duration'].append(duration)\n",
    "                        output[m+'_wdg_loss'].append(loss)\n",
    "                        print(f\"Accuracy={accuracy: 7.2f}    Duration={duration: 7.2f}\")\n",
    "                verification_array = np.array([m+'_sdg_Accuracy',m+'_sdg_Duration',m+'_sdg_loss',m+'_wdg_Accuracy',m+'_wdg_Duration',m+'_wdg_loss'])      \n",
    "                for casse in verification_array:\n",
    "                    if output[casse] == []:\n",
    "                        output[casse].append('-')\n",
    "            except:\n",
    "                raise\n",
    "                print('-')\n",
    "                print(\"comment c'est fucking possible que ce l'entrainement ne marche pas\")\n",
    "            \n",
    "    return output, imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b497c82-e1b9-49cd-89e8-eac20d57eabb",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c63152-81e5-4563-83ca-956d8b9687b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwk.chrono_start()\n",
    "\n",
    "print('\\n---- Run','-'*50)\n",
    "\n",
    "\n",
    "# ---- Data augmentation or not\n",
    "\n",
    "    \n",
    "# ---- Run\n",
    "#\n",
    "output, imgs = multi_run(datasets, \n",
    "                   models,\n",
    "                   with_datagen  = with_datagen,\n",
    "                   scale         = scale,\n",
    "                   batch_size    = batch_size,\n",
    "                   epochs        = epochs,\n",
    "                   fit_verbosity = fit_verbosity,\n",
    "                   tag_id        = tag_id)\n",
    "\n",
    "# ---- Save report\n",
    "#\n",
    "report={}\n",
    "report['output']=output\n",
    "report['description'] = f'scale={scale} batch_size={batch_size} epochs={epochs} data_aug={with_datagen}'\n",
    "\n",
    "report_name=f'{run_dir}/report_{tag_id}.json'\n",
    "\n",
    "with open(report_name, 'w') as file:\n",
    "    json.dump(report, file, indent=4)\n",
    "\n",
    "print('\\nReport saved as ',report_name)\n",
    "\n",
    "pwk.chrono_show()\n",
    "print('-'*59)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe24b6-ad48-4056-bfef-b403770010c7",
   "metadata": {},
   "source": [
    "## show report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5807c0-9280-45e6-98b9-c83193db1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dir = run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f6b7f-873e-4844-82e9-790cd6a3c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = (s == s.max())\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "def show_report(file):\n",
    "    # ---- Read json file\n",
    "    with open(file) as infile:\n",
    "        dict_report = json.load( infile )\n",
    "    output      = dict_report['output']\n",
    "    description = dict_report['description']\n",
    "    # ---- about\n",
    "    pwk.subtitle(f'Report : {Path(file).stem}')\n",
    "    print(    \"Desc.  : \",description,'\\n')\n",
    "    # ---- Create a pandas\n",
    "    report       = pd.DataFrame (output)\n",
    "    col_accuracy = [ c for c in output.keys() if c.endswith('Accuracy')]\n",
    "    col_duration = [ c for c in output.keys() if c.endswith('Duration')]\n",
    "    # ---- Build formats\n",
    "    lambda_acc = lambda x : '{:.2f} %'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n",
    "    lambda_dur = lambda x : '{:.1f} s'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n",
    "    formats = {'Size':'{:.2f} Mo'}\n",
    "    for c in col_accuracy:   \n",
    "        formats[c]=lambda_acc\n",
    "    for c in col_duration:\n",
    "        formats[c]=lambda_dur\n",
    "    t=report.style.highlight_max(subset=col_accuracy).format(formats).hide_index()\n",
    "    display(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647edae-2c97-47c6-8ef3-d7c6e68fe448",
   "metadata": {},
   "source": [
    "## Step 3 - Reports display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950b1e7-3537-4a3a-a91a-465eb46e9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in glob.glob(f'{report_dir}/*.json'):\n",
    "file = report_name\n",
    "show_report(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911fcb6-e831-4c9f-bb0b-ce3093fed152",
   "metadata": {},
   "source": [
    "## model evaluate matrix de confusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c5546-4736-480e-8aff-4c54bce654af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir(f'{run_dir}\\models_{tag_id}')\n",
    "print(models) # tu choisis un des modeles listés et le mettre dans le variable model_choisit au dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56563e-9d50-4c7c-bbf5-20b12768a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choisit = 'model_wdg_img_64_e15bs64sc0.5code2_700_11234.h5'\n",
    "model_choisit = f'{run_dir}\\models_{tag_id}\\{model_choisit}'\n",
    "model_evaluated = tf.keras.models.load_model(model_choisit) #load ton model choisite, tu peux aussi copie coller la chemin absolute ou le model est stocké"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88fe71-0114-4ee6-a274-6a2d30cdafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluated.summary() # son structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf8026-e045-471c-9434-0ef18e48c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test_final = imgs[4]\n",
    "ID_test_final = imgs[5]\n",
    "score = model_evaluated.evaluate(img_test_final, ID_test_final, verbose=0) #evaluation de ce models\n",
    "\n",
    "print('Test loss     :', score[0])\n",
    "print('Test accuracy :', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad82cf-6a22-4667-9746-4eeacf6bb555",
   "metadata": {},
   "source": [
    "## erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e94371-335a-43d6-802d-5136a79713d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sigmoid = model_evaluated.predict(img_test_final)\n",
    "ID_pred    = np.argmax(y_sigmoid, axis=-1)\n",
    "\n",
    "pwk.plot_images(img_test_final, ID_test_final, range(0,200), columns=12, x_size=1, y_size=1, y_pred=ID_pred, save_as='04-predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f0bdd-cf22-4270-85e0-6529b1a0195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwk.plot_confusion_matrix(ID_test_final,ID_pred,range(11),normalize=True, save_as='06-confusion-matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23271a33-360c-4bfc-8d5b-a17b175c41fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
